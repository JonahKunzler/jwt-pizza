Summary:

Between the hour of 10:35 and 11:15 UTC on April 8, 2025, 300 users encountered failed pizza orders and slow response times. The event was triggered by a chaos test at 10:35 UTC. The chaos test contained a simulated database connection failure to test system resilience.

A configuration issue in the database connection pool caused the system to reject requests, leading to a partial outage. The event was detected by Grafana IRM alerts. The team started working on the event by restarting the database service and reconfiguring the connection pool. This high-severity incident affected 60% of users.

There was further impact as noted by 300 users being unable to place orders for 40 minutes in relation to this incident.


Detection:

This incident was detected when the "DatasourceNoData" alert was triggered and the on-call DevOps team were paged.

Next, a secondary on-call engineer was paged, because the first engineer didn’t know the database system, delaying the response by 2 minutes.

A direct database connection alert will be set up by the Monitoring team so that we can find issues in 1 minute instead of 2.


Impact:

For 40 minutes between 10:35 UTC and 11:15 UTC on 04/08/25, a partial outage on the JWT Pizza website stopped our users from ordering pizzas and slowed down the site.

This incident affected 300 customers (60% of active users), who experienced error messages and delays of up to 15 seconds when trying to order.

Timeline:

All times are in UTC.

10:35 - Chaos test starts, breaking the database connection in the JWT Pizza application.
10:35 - First "DatasourceNoData" alert (#584) is triggered by Grafana IRM, notifying the DevOps team.
10:36 - More "DatasourceNoData" alerts (#588, #589, #590) and CPU usage alerts (#585, #586) are triggered, showing a bigger issue.
10:37 - A second DevOps engineer is paged to help with the database problem.
10:40 - DevOps team begins checking the database connection issues.
10:45 - Team finds the database connection pool is not working right, causing the outage.
10:55 - Team restarts the database to fix the connection.
11:00 - Team stops the chaos test to avoid more problems.
11:10 - Grafana alerts stop, and the website starts working better for users.
11:15 - The incident ends, with the website fully back to normal and users able to order pizzas again.

Response:

After receiving a page at 10:35 UTC, the on-call DevOps engineer came online at 10:36 UTC in Grafana OnCall.
This engineer did not have a background in the database system, so a second alert was sent at 10:37 UTC to the secondary on-call engineer, who came into the room at 10:40 UTC. The delay in finding someone who knew the database slowed the response by 2 minutes.

Root Cause:

A wrong setting in the database connection pool caused it to stop working during the chaos test, leading to "DatasourceNoData" errors, and there wasn’t a good way to see the problem quickly.

Resolution:

The website was fixed by restarting the database at 10:55 UTC, which got the connection working again, and stopping the chaos test at 11:00 UTC so it wouldn’t cause more problems. By 11:15 UTC, the incident was over because the Grafana alerts stopped, and people could order pizzas normally again. The team knew it was fixed because the website loaded fast and orders went through without errors. We could make this faster by setting up the database to restart on its own if it breaks, which would cut the fix time from 40 minutes to 20 minutes.

Prevention:

This same database problem happened before in incident INC-2025-03-12-1, where 100 people couldn’t order pizzas. Added a small monitor to watch the database, but it didn’t catch this bigger issue. This incident happened again because the monitor couldn’t see when the database connection pool stopped working.

Action Items:

Fix the database connection pool settings to handle more users.
Add a new Grafana alert to watch the database connection pool.
Set up the database to restart on its own if it stops working.

